README - Elastic Weight Consolidation (EWC) Demo

============= Objectif du projet =============

Ce projet a Ã©tÃ© rÃ©alisÃ© dans le cadre de lâ€™Ã©valuation du cours de Deep Learning 2025â€“2026 (ISAE-Supaero).
Lâ€™objectif est de :

- Expliquer clairement le phÃ©nomÃ¨ne dâ€™oubli catastrophique dans lâ€™apprentissage sÃ©quentiel.
- ImplÃ©menter Elastic Weight Consolidation (EWC) sur MNIST et Permuted MNIST.
- Fournir une dÃ©monstration de code simple et rÃ©utilisable.
- Proposer une extension originale au-delÃ  de la reproduction du papier de Kirkpatrick et al. (2017).

Le projet comprend :

- une expÃ©rience principale (2 ou 3 tÃ¢ches)
- une simulation longue (5 tÃ¢ches)
- une extension expÃ©rimentale (effet du paramÃ¨tre lambda)

ğŸ“¦ Structure du repository
ewc-demo/
â”‚
â”œâ”€â”€ ewc_mnist.py              # ExpÃ©riences principales : 2 tÃ¢ches et 3 tÃ¢ches
â”œâ”€â”€ ewc_lambda_sweep.py       # Extension originale : Ã©tude systÃ©matique du paramÃ¨tre lambda
â”œâ”€â”€ ewc_5tasks_demo.py        # Apprentissage sÃ©quentiel long (5 tÃ¢ches)
â”‚
â”œâ”€â”€ ewc_results.png           # RÃ©sultats visuels pour 2 tÃ¢ches
â”œâ”€â”€ ewc_3tasks_curves.png     # Courbes dâ€™Ã©volution pour 3 tÃ¢ches
â”œâ”€â”€ ewc_5tasks_heatmap.png    # Heatmap complÃ¨te pour 5 tÃ¢ches
â”œâ”€â”€ lambda_sweep.png          # Courbes dâ€™Ã©volution pour 3 tÃ¢ches en fonction de lambda
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

============= Installation =============

1. CrÃ©er un environnement virtuel

python -m venv .venv

3. Activer lâ€™environnement

source venv\Scripts\activate

5. Installer les dÃ©pendances

pip install -r requirements.txt

============= 1. ExpÃ©rience principale : EWC sur deux ou trois tÃ¢ches =============

Le fichier ewc_mnist.py permet dâ€™exÃ©cuter :

- un apprentissage sÃ©quentiel classique (naÃ¯f) ou avec EWC
- avec deux tÃ¢ches : MNIST â†’ Permuted MNIST
- ou trois tÃ¢ches : MNIST â†’ Permuted1 â†’ Permuted2

Il gÃ©nÃ¨re automatiquement :

- des rÃ©sultats chiffrÃ©s en console
- des graphiques comparant naive vs EWC

â–¶ï¸ Lancer lâ€™expÃ©rience 2 tÃ¢ches

Dans ewc_mnist.py, laisser activÃ© :

if __name__ == "__main__":
    run_experiment(
        epochs_a=5,
        epochs_b=5,
        lr=1e-3,
        lambda_ewc=1000.0,
        batch_size=128
    )


Puis exÃ©cuter :
python ewc_mnist.py

â–¶ï¸ Lancer lâ€™expÃ©rience 3 tÃ¢ches

DÃ©commenter :
run_3tasks(...)

Et exÃ©cuter :
python ewc_mnist.py

============= 2. Extension originale : Ã©tude du paramÃ¨tre lambda =============

Le fichier ewc_lambda_sweep.py explore plusieurs valeurs de lambda :

lambda âˆˆ {0, 10, 100, 300, 1000, 3000}

Pour chaque lambda, le modÃ¨le apprend successivement : A â†’ B â†’ C

Et on mesure : accuracy sur A, accuracy sur B et accuracy sur C.

Le script gÃ©nÃ¨re le graphique lambda_sweep.png, montrant lâ€™impact du lambda sur la stabilitÃ© et la plasticitÃ© du modÃ¨le.

â–¶ï¸ Lancer lâ€™expÃ©rience
python ewc_lambda_sweep.py

============= 3. Simulation avancÃ©e : apprentissage long sur 5 tÃ¢ches =============

Le fichier ewc_5tasks_demo.py montre comment EWC se comporte dans un contexte plus rÃ©aliste de continual learning avec :

- T1 : MNIST normal
- T2â€“T5 : MNIST permutÃ© (4 permutations diffÃ©rentes)

Le script compare :

- modÃ¨le naÃ¯f (lambda = 0)
- EWC (lambda = 1000)

Il gÃ©nÃ¨re une heatmap : ewc_5tasks_heatmap.png

Cette heatmap montre un oubli cumulatif trÃ¨s fort dans le modÃ¨le naÃ¯f et une stabilitÃ© remarquable avec EWC (presque aucune dÃ©gradation aprÃ¨s 5 tÃ¢ches).

â–¶ï¸ Lancer lâ€™expÃ©rience
python ewc_5tasks_demo.py

============= RÃ©sumÃ© pÃ©dagogique =============

1. Lâ€™oubli catastrophique

Lorsquâ€™un rÃ©seau apprend des tÃ¢ches sÃ©quentielles, les nouvelles mises Ã  jour de gradient Ã©crasent les connaissances nÃ©cessaires aux anciennes tÃ¢ches.
RÃ©sultat : perte massive de performance sur les premiÃ¨res tÃ¢ches.

2. Le principe dâ€™EWC

EWC ajoute une pÃ©nalitÃ© quadratique sur les poids les plus importants (estimÃ©s avec la matrice de Fisher) :

Loss_total = Loss_task_B + (lambda/2) * Î£[F_i * (Î¸_i - Î¸_i*)Â²]

Cela force le rÃ©seau Ã  conserver les paramÃ¨tres critiques pour les anciennes tÃ¢ches (stabilitÃ©) et apprendre les nouvelles tÃ¢ches via les paramÃ¨tres moins critiques (plasticitÃ©).

3. Ce que montrent les expÃ©riences

ExpÃ©rience 2 tÃ¢ches

- modÃ¨le naÃ¯f : perte sÃ©vÃ¨re sur la tÃ¢che A
- modÃ¨le EWC : protection trÃ¨s forte de la performance

ExpÃ©rience 3 tÃ¢ches

- mise en Ã©vidence du compromis stabilitÃ©-plasticitÃ©
- meilleure comprÃ©hension du rÃ´le de lambda

Extension : sweep de lambda

- lambda trop faible â†’ oubli important
- lambda intermÃ©diaire (300â€“1000) â†’ compromis optimal
- lambda trÃ¨s fort â†’ modÃ¨le trop rigide

ExpÃ©rience 5 tÃ¢ches

- modÃ¨le naÃ¯f â†’ effondrement cumulatif (T1 tombe Ã  35%)
- EWC â†’ toutes les tÃ¢ches restent autour de 93â€“96%
- dÃ©monstration claire de continual learning stabilisÃ©
